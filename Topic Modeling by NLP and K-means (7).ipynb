{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought of as a form of text mining â€“ a way to obtain recurring patterns of words in textual material.\n",
    "In this project I will use unsupervised techniques, to cluster/ group reviews to identify main topics/ ideas in the sea of text. This will be applicable to any textual reviews but I will focus on twitter data which is more real world and more complex than reviews obtained from review or survey forms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3DYk3YENh2N"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re # remove regex\n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fuVO1rS7Nsyh",
    "outputId": "a48aaccb-3f00-420d-a3ee-8a603586ed81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"tweets.csv\",encoding = 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tLxgbtCBOjQz",
    "outputId": "b0282410-0bc6-43e1-dacd-d35534d273ad"
   },
   "outputs": [],
   "source": [
    "unique_text = df.tweet.unique()\n",
    "print(len(unique_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7VdPI66Oomu"
   },
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any @ mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLg23YJSOqTh"
   },
   "outputs": [],
   "source": [
    "df['Clean_text'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")\n",
    "df['Clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwG6d92uOr9l"
   },
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df['Clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbHgyvLOOtrC"
   },
   "outputs": [],
   "source": [
    "df[\"Clean_text\"]= df[\"Clean_text\"].str.lower() \n",
    "df['Clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "oRG6wwo9OvEH",
    "outputId": "bd707a8b-554c-4103-ab59-463e91b2c061"
   },
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "x59SKuK-Owzq",
    "outputId": "5bfb76b4-3f83-4b3b-92cd-f720a81e5e10"
   },
   "outputs": [],
   "source": [
    "tokenized_tweet = df['Clean_text'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmImRqatOzS9"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "df['Clean_text'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "Nwu1eJHKO1Fk",
    "outputId": "21eece25-ebac-438b-ea58-81705efeb500"
   },
   "outputs": [],
   "source": [
    "df.loc[:,('Clean_text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAqCcfirO26F"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['Clean_text'], keep = 'first',inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dj_oKMwNO5Bg"
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "vWbISfuIO7Kx",
    "outputId": "bed365e1-5640-4e7a-d057-0a9fc9cbcafd"
   },
   "outputs": [],
   "source": [
    "df['Clean_text_length'] = df['Clean_text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Clean_text_length']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Am8MmcYO9E4",
    "outputId": "ee91b1f9-339a-41d7-9d62-18d0990aae84"
   },
   "outputs": [],
   "source": [
    "df[df['Clean_text_length']==0]['Clean_text'] ## Looks like these are tweets with different languages or just hastags.\n",
    "# We can simply drop these tweets\n",
    "indexes = df[df['Clean_text_length']==0]['Clean_text'].index\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MFMUXRqO_U3"
   },
   "outputs": [],
   "source": [
    "df.drop(index = indexes,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "4VwFO5UMPBZd",
    "outputId": "e26f731b-fae9-479c-d18e-64908d242188"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "Y17F5Ad4PCu-",
    "outputId": "dc2c1d27-dcc7-4596-912e-b9b8138adc49"
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "4p9FGNmrPFbh",
    "outputId": "d8918539-f051-415e-bdc7-47069b4e6130"
   },
   "outputs": [],
   "source": [
    "df['Clean_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer:\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary\\\n",
    "Number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "O-pss7rtPHOQ",
    "outputId": "9eb583de-db60-4993-df12-5fa8cc5f237b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word',ngram_range=(1,1), stop_words='english', min_df = 0.0001, max_df=0.7)\n",
    "count_vect.fit(df['Clean_text'])\n",
    "desc_matrix = count_vect.transform(df[\"Clean_text\"])\n",
    "desc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VQrxMV3c4ZO"
   },
   "outputs": [],
   "source": [
    "def wordcloud(cluster):\n",
    "  # combining the image with the dataset\n",
    "  Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n",
    "\n",
    "  # We use the ImageColorGenerator library from Wordcloud \n",
    "  # Here we take the color of the image and impose it over our wordcloud\n",
    "  image_colors = ImageColorGenerator(Mask)\n",
    "\n",
    "  # Now we use the WordCloud function from the wordcloud library \n",
    "  wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(cluster)\n",
    "\n",
    "  # Size of the image generated \n",
    "  plt.figure(figsize=(10,20))\n",
    "\n",
    "  # Here we recolor the words from the dataset to the image's color\n",
    "  # recolor just recolors the default colors to the image's blue color\n",
    "  # interpolation is used to smooth the image generated \n",
    "  plt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\n",
    "\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topics(df, desc_matrix, num_clusters):\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(desc_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    tweets = {'Tweet': df[\"Clean_text\"].tolist(), 'Cluster': clusters}\n",
    "    frame = pd.DataFrame(tweets, index = [clusters])\n",
    "    print(frame['Cluster'].value_counts())\n",
    "\n",
    "    \n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_words = ' '.join(text for text in frame[frame['Cluster'] == cluster]['Tweet'])\n",
    "        wordcloud(cluster_words)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_topics(df, desc_matrix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_topics(df, desc_matrix, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 seems like a good number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_csv('clustered_tweets.csv') "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "knn_8_clusters.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
